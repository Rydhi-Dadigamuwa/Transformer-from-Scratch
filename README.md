# ğŸ§  Build Transformer from Scratch

In this project, I built a **Transformer network** â€” an **encoder-only architecture** with **two encoder blocks**, each containing **two multi-head attention heads**, specifically designed for **sentiment analysis**.  
This implementation is built **entirely from scratch**, without using any prebuilt Transformer layers, to provide a clear understanding of the architectureâ€™s inner mechanics.

---

## ğŸ“˜ Overview

Transformers have redefined modern NLP by relying purely on **attention mechanisms** instead of recurrence or convolution.  
This project demonstrates how to **build an encoder-only Transformer** for **sentiment classification** from the ground up using TensorFlow and NumPy.

The notebook explains and implements:

1. Token and Positional Embeddings  
2. Scaled Dot-Product Attention  
3. Multi-Head Attention (2 heads per encoder)  
4. Feed-Forward Networks  
5. Layer Normalization and Residual Connections  
6. Encoder Stack (2 layers)  
7. Classification Head for Sentiment Prediction  
8. Training Loop and Evaluation

---

## ğŸ—ï¸ Project Structure

build-transformer-from-scratch

â”œâ”€â”€ Build_Transformer_from_Scratch.ipynb             # Main Jupyter notebook implementing the Transformer model  
â”œâ”€â”€ README.md                                        # Project documentation  
â”œâ”€â”€ IMDB_Dataset.csv                                 # Kaggle dataset (https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)  
â”œâ”€â”€ Transformer_Sentiment_Model.h5                   # Saved encoder-only Transformer model (H5 format)  
â”œâ”€â”€ Transformer_Sentiment_Model.keras                # Saved encoder-only Transformer model (Keras format)  
â””â”€â”€ model_architecture.png                           # Model architecture visualization  



---

## âš™ï¸ Features

- âœ… **Encoder-only Transformer** designed for sentiment analysis  
- ğŸ§© **Two encoder blocks**, each with **two multi-head attention heads**  
- ğŸ§  Built **completely from scratch** using TensorFlow/Keras low-level APIs  
- ğŸ“Š Includes training and evaluation for classification  
- ğŸ’¬ Easy to extend to other NLP tasks  

---

## ğŸ§© Key Components

| Component | Description |
|------------|-------------|
| **Embedding Layer** | Converts tokens into dense vector representations. |
| **Positional Encoding** | Adds position information to embeddings using sine and cosine functions. |
| **Scaled Dot-Product Attention** | Calculates the attention scores between queries, keys, and values. |
| **Multi-Head Attention** | Allows the model to focus on different parts of the sequence simultaneously. |
| **Feed-Forward Network** | Applies non-linear transformations to enrich learned features. |
| **Encoder Block** | Combines attention and feed-forward sublayers with residual connections. |
| **Classifier Head** | Outputs sentiment class probabilities. |

---
